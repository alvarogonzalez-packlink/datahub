"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[157],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>m});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},u=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=c(a),m=r,h=p["".concat(l,".").concat(m)]||p[m]||d[m]||o;return a?n.createElement(h,i(i({ref:t},u),{},{components:a})):n.createElement(h,i({ref:t},u))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=a[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},18679:(e,t,a)=>{a.d(t,{Z:()=>i});var n=a(67294),r=a(86010);const o="tabItem_Ymn6";function i(e){let{children:t,hidden:a,className:i}=e;return n.createElement("div",{role:"tabpanel",className:(0,r.Z)(o,i),hidden:a},t)}},34259:(e,t,a)=>{a.d(t,{Z:()=>m});var n=a(83117),r=a(67294),o=a(86010),i=a(51048),s=a(33609),l=a(1943),c=a(72957);const u="tabList__CuJ",d="tabItem_LNqP";function p(e){const{lazy:t,block:a,defaultValue:i,values:p,groupId:m,className:h}=e,b=r.Children.map(e.children,(e=>{if((0,r.isValidElement)(e)&&"value"in e.props)return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})),f=p??b.map((e=>{let{props:{value:t,label:a,attributes:n}}=e;return{value:t,label:a,attributes:n}})),g=(0,s.l)(f,((e,t)=>e.value===t.value));if(g.length>0)throw new Error(`Docusaurus error: Duplicate values "${g.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`);const v=null===i?i:i??b.find((e=>e.props.default))?.props.value??b[0].props.value;if(null!==v&&!f.some((e=>e.value===v)))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${v}" but none of its children has the corresponding value. Available values are: ${f.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);const{tabGroupChoices:y,setTabGroupChoices:k}=(0,l.U)(),[P,E]=(0,r.useState)(v),x=[],{blockElementScrollPositionUntilNextRender:w}=(0,c.o5)();if(null!=m){const e=y[m];null!=e&&e!==P&&f.some((t=>t.value===e))&&E(e)}const _=e=>{const t=e.currentTarget,a=x.indexOf(t),n=f[a].value;n!==P&&(w(t),E(n),null!=m&&k(m,String(n)))},D=e=>{let t=null;switch(e.key){case"Enter":_(e);break;case"ArrowRight":{const a=x.indexOf(e.currentTarget)+1;t=x[a]??x[0];break}case"ArrowLeft":{const a=x.indexOf(e.currentTarget)-1;t=x[a]??x[x.length-1];break}}t?.focus()};return r.createElement("div",{className:(0,o.Z)("tabs-container",u)},r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":a},h)},f.map((e=>{let{value:t,label:a,attributes:i}=e;return r.createElement("li",(0,n.Z)({role:"tab",tabIndex:P===t?0:-1,"aria-selected":P===t,key:t,ref:e=>x.push(e),onKeyDown:D,onClick:_},i,{className:(0,o.Z)("tabs__item",d,i?.className,{"tabs__item--active":P===t})}),a??t)}))),t?(0,r.cloneElement)(b.filter((e=>e.props.value===P))[0],{className:"margin-top--md"}):r.createElement("div",{className:"margin-top--md"},b.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==P})))))}function m(e){const t=(0,i.Z)();return r.createElement(p,(0,n.Z)({key:String(t)},e))}},80488:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>u,contentTitle:()=>l,default:()=>m,frontMatter:()=>s,metadata:()=>c,toc:()=>d});var n=a(83117),r=(a(67294),a(3905)),o=a(34259),i=a(18679);const s={title:"But First, Semantics: Upsert versus Patch",slug:"/advanced/patch",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/advanced/patch.md"},l="But First, Semantics: Upsert versus Patch",c={unversionedId:"docs/advanced/patch",id:"docs/advanced/patch",title:"But First, Semantics: Upsert versus Patch",description:"Why Would You Use Patch",source:"@site/genDocs/docs/advanced/patch.md",sourceDirName:"docs/advanced",slug:"/advanced/patch",permalink:"/docs/advanced/patch",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/advanced/patch.md",tags:[],version:"current",frontMatter:{title:"But First, Semantics: Upsert versus Patch",slug:"/advanced/patch",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/advanced/patch.md"},sidebar:"overviewSidebar",previous:{title:"Java Emitter",permalink:"/docs/metadata-integration/java/as-a-library"},next:{title:"Dataset",permalink:"/docs/api/tutorials/datasets"}},u={},d=[{value:"Why Would You Use Patch",id:"why-would-you-use-patch",level:2},{value:"How To Use Patch",id:"how-to-use-patch",level:2},{value:"Add Custom Properties",id:"add-custom-properties",level:3},{value:"Add and Remove Custom Properties",id:"add-and-remove-custom-properties",level:3},{value:"Add Data Job Lineage",id:"add-data-job-lineage",level:3},{value:"Add Properties to Dataset",id:"add-properties-to-dataset",level:3}],p={toc:d};function m(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"but-first-semantics-upsert-versus-patch"},"But First, Semantics: Upsert versus Patch"),(0,r.kt)("h2",{id:"why-would-you-use-patch"},"Why Would You Use Patch"),(0,r.kt)("p",null,"By default, most of the SDK tutorials and API-s involve applying full upserts at the aspect level. This means that typically, when you want to change one field within an aspect without modifying others, you need to do a read-modify-write to not overwrite existing fields.\nTo support these scenarios, DataHub supports PATCH based operations so that targeted changes to single fields or values within arrays of fields are possible without impacting other existing metadata."),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"Currently, PATCH support is only available for a selected set of aspects, so before pinning your hopes on using PATCH as a way to make modifications to aspect values, confirm whether your aspect supports PATCH semantics. The complete list of Aspects that are supported are maintained ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/9588440549f3d99965085e97b214a7dabc181ed2/entity-registry/src/main/java/com/linkedin/metadata/models/registry/template/AspectTemplateEngine.java#L24"},"here"),". In the near future, we do have plans to automatically support PATCH semantics for aspects by default.")),(0,r.kt)("h2",{id:"how-to-use-patch"},"How To Use Patch"),(0,r.kt)("p",null,"Examples for using Patch are sprinkled throughout the API guides.\nHere's how to find the appropriate classes for the language for your choice."),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"Java",label:"Java SDK",mdxType:"TabItem"},(0,r.kt)("p",null,"The Java Patch builders are aspect-oriented and located in the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/tree/master/metadata-integration/java/datahub-client/src/main/java/datahub/client/patch"},"datahub-client")," module under the ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub.client.patch")," namespace."),(0,r.kt)("p",null,"Here are a few illustrative examples using the Java Patch builders:"),(0,r.kt)("h3",{id:"add-custom-properties"},"Add Custom Properties"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'# Inlined from /metadata-integration/java/examples/src/main/java/io/datahubproject/examples/DatasetCustomPropertiesAdd.java\npackage io.datahubproject.examples;\n\nimport com.linkedin.common.urn.UrnUtils;\nimport datahub.client.MetadataWriteResponse;\nimport datahub.client.patch.dataset.DatasetPropertiesPatchBuilder;\nimport datahub.client.rest.RestEmitter;\nimport java.io.IOException;\nimport com.linkedin.mxe.MetadataChangeProposal;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport lombok.extern.slf4j.Slf4j;\n\n\n@Slf4j\nclass DatasetCustomPropertiesAdd {\n\n  private DatasetCustomPropertiesAdd() {\n\n  }\n\n  /**\n   * Adds properties to an existing custom properties aspect without affecting any existing properties\n   * @param args\n   * @throws IOException\n   * @throws ExecutionException\n   * @throws InterruptedException\n   */\n    public static void main(String[] args) throws IOException, ExecutionException, InterruptedException {\n      MetadataChangeProposal datasetPropertiesProposal = new DatasetPropertiesPatchBuilder()\n          .urn(UrnUtils.toDatasetUrn("hive", "fct_users_deleted", "PROD"))\n          .addCustomProperty("cluster_name", "datahubproject.acryl.io")\n          .addCustomProperty("retention_time", "2 years")\n          .build();\n\n      String token = "";\n      RestEmitter emitter = RestEmitter.create(\n          b -> b.server("http://localhost:8080")\n              .token(token)\n      );\n      try {\n        Future<MetadataWriteResponse> response = emitter.emit(datasetPropertiesProposal);\n\n        System.out.println(response.get().getResponseContent());\n      } catch (Exception e) {\n        log.error("Failed to emit metadata to DataHub", e);\n        throw e;\n      } finally {\n        emitter.close();\n      }\n\n    }\n\n}\n\n\n\n')),(0,r.kt)("h3",{id:"add-and-remove-custom-properties"},"Add and Remove Custom Properties"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'# Inlined from /metadata-integration/java/examples/src/main/java/io/datahubproject/examples/DatasetCustomPropertiesAddRemove.java\npackage io.datahubproject.examples;\n\nimport com.linkedin.common.urn.UrnUtils;\nimport com.linkedin.mxe.MetadataChangeProposal;\nimport datahub.client.MetadataWriteResponse;\nimport datahub.client.patch.dataset.DatasetPropertiesPatchBuilder;\nimport datahub.client.rest.RestEmitter;\nimport java.io.IOException;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport lombok.extern.slf4j.Slf4j;\n\n\n@Slf4j\nclass DatasetCustomPropertiesAddRemove {\n\n  private DatasetCustomPropertiesAddRemove() {\n\n  }\n\n  /**\n   * Applies Add and Remove property operations on an existing custom properties aspect without\n   * affecting any other properties\n   * @param args\n   * @throws IOException\n   * @throws ExecutionException\n   * @throws InterruptedException\n   */\n    public static void main(String[] args) throws IOException, ExecutionException, InterruptedException {\n      MetadataChangeProposal datasetPropertiesProposal = new DatasetPropertiesPatchBuilder()\n          .urn(UrnUtils.toDatasetUrn("hive", "fct_users_deleted", "PROD"))\n          .addCustomProperty("cluster_name", "datahubproject.acryl.io")\n          .removeCustomProperty("retention_time")\n          .build();\n\n      String token = "";\n      RestEmitter emitter = RestEmitter.create(\n          b -> b.server("http://localhost:8080")\n              .token(token)\n      );\n      try {\n        Future<MetadataWriteResponse> response = emitter.emit(datasetPropertiesProposal);\n\n        System.out.println(response.get().getResponseContent());\n      } catch (Exception e) {\n        log.error("Failed to emit metadata to DataHub", e);\n        throw e;\n      } finally {\n        emitter.close();\n      }\n\n    }\n\n}\n\n\n\n')),(0,r.kt)("h3",{id:"add-data-job-lineage"},"Add Data Job Lineage"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'# Inlined from /metadata-integration/java/examples/src/main/java/io/datahubproject/examples/DataJobLineageAdd.java\npackage io.datahubproject.examples;\n\nimport com.linkedin.common.urn.DataJobUrn;\nimport com.linkedin.common.urn.DatasetUrn;\nimport com.linkedin.common.urn.UrnUtils;\nimport datahub.client.MetadataWriteResponse;\nimport datahub.client.patch.datajob.DataJobInputOutputPatchBuilder;\nimport datahub.client.rest.RestEmitter;\nimport java.io.IOException;\nimport com.linkedin.mxe.MetadataChangeProposal;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport lombok.extern.slf4j.Slf4j;\n\n\n@Slf4j\nclass DataJobLineageAdd {\n\n  private DataJobLineageAdd() {\n\n  }\n\n  /**\n   * Adds lineage to an existing DataJob without affecting any lineage\n   * @param args\n   * @throws IOException\n   * @throws ExecutionException\n   * @throws InterruptedException\n   */\n  public static void main(String[] args) throws IOException, ExecutionException, InterruptedException {\n    String token = "";\n    try (RestEmitter emitter = RestEmitter.create(\n        b -> b.server("http://localhost:8080")\n            .token(token)\n    )) {\n      MetadataChangeProposal dataJobIOPatch = new DataJobInputOutputPatchBuilder().urn(UrnUtils\n              .getUrn("urn:li:dataJob:(urn:li:dataFlow:(airflow,dag_abc,PROD),task_456)"))\n          .addInputDatasetEdge(DatasetUrn.createFromString("urn:li:dataset:(urn:li:dataPlatform:kafka,SampleKafkaDataset,PROD)"))\n          .addOutputDatasetEdge(DatasetUrn.createFromString("urn:li:dataset:(urn:li:dataPlatform:kafka,SampleHiveDataset,PROD)"))\n          .addInputDatajobEdge(DataJobUrn.createFromString("urn:li:dataJob:(urn:li:dataFlow:(airflow,dag_abc,PROD),task_123)"))\n          .addInputDatasetField(UrnUtils.getUrn(\n              "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_deleted,PROD),user_id)"))\n          .addOutputDatasetField(UrnUtils.getUrn(\n              "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_created,PROD),user_id)"))\n          .build();\n\n      Future<MetadataWriteResponse> response = emitter.emit(dataJobIOPatch);\n\n      System.out.println(response.get().getResponseContent());\n    } catch (Exception e) {\n      log.error("Failed to emit metadata to DataHub", e);\n      throw new RuntimeException(e);\n    }\n\n  }\n\n}\n\n\n\n'))),(0,r.kt)(i.Z,{value:"Python",label:"Python SDK",default:!0,mdxType:"TabItem"},(0,r.kt)("p",null,"The Python Patch builders are entity-oriented and located in the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/tree/9588440549f3d99965085e97b214a7dabc181ed2/metadata-ingestion/src/datahub/specific"},"metadata-ingestion")," module and located in the ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub.specific")," module."),(0,r.kt)("p",null,"Here are a few illustrative examples using the Python Patch builders:"),(0,r.kt)("h3",{id:"add-properties-to-dataset"},"Add Properties to Dataset"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/dataset_add_properties.py\nimport logging\nfrom typing import Union\n\nfrom datahub.configuration.kafka import KafkaProducerConnectionConfig\nfrom datahub.emitter.kafka_emitter import DatahubKafkaEmitter, KafkaEmitterConfig\nfrom datahub.emitter.mce_builder import make_dataset_urn\nfrom datahub.emitter.rest_emitter import DataHubRestEmitter\nfrom datahub.specific.dataset import DatasetPatchBuilder\n\nlog = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\n\n# Get an emitter, either REST or Kafka, this example shows you both\ndef get_emitter() -> Union[DataHubRestEmitter, DatahubKafkaEmitter]:\n    USE_REST_EMITTER = True\n    if USE_REST_EMITTER:\n        gms_endpoint = "http://localhost:8080"\n        return DataHubRestEmitter(gms_server=gms_endpoint)\n    else:\n        kafka_server = "localhost:9092"\n        schema_registry_url = "http://localhost:8081"\n        return DatahubKafkaEmitter(\n            config=KafkaEmitterConfig(\n                connection=KafkaProducerConnectionConfig(\n                    bootstrap=kafka_server, schema_registry_url=schema_registry_url\n                )\n            )\n        )\n\n\ndataset_urn = make_dataset_urn(platform="hive", name="fct_users_created", env="PROD")\n\nwith get_emitter() as emitter:\n    for patch_mcp in (\n        DatasetPatchBuilder(dataset_urn)\n        .add_custom_property("cluster_name", "datahubproject.acryl.io")\n        .add_custom_property("retention_time", "2 years")\n        .build()\n    ):\n        emitter.emit(patch_mcp)\n\n\nlog.info(f"Added cluster_name, retention_time properties to dataset {dataset_urn}")\n\n')))))}m.isMDXComponent=!0}}]);